# Stock Prediction Model

The project focus on comparitive analysis of modern activation with common activation over 3 architecture for LSTM Model. Here , We Have implemented a basic LSTM model, a Dense LSTM model and a Hybrid LSTM+CNN model.


## Architecture and Activation

- **LSTM Model:** An effective recurrent neural network variation, Long Short-Term Memory (LSTM) is well-suited for sequential data processing applications such as time series prediction and natural language processing because it effectively mitigates the vanishing gradient problem.

- **SELU Activation:** The Scaled Exponential Linear Unit (SELU) is a self-normalizing activation function that allows neural networks to normalize and stabilize while training. Improved performance and quicker convergence are possible outcomes of this, particularly in deep networks.

- **RELU Activation:** The Rectified Linear Unit (ReLU) is a popular activation function that incorporates nonlinearity by returning the input for positive values and zero for negative values. It improves the expressiveness of the model and is now the standard option for many deep learning applications.


- **GELU Activation:** By combining features of the Gaussian distribution, the Gaussian Error Linear Unit (GELU) activation function is intended to represent the non-linearity of data. GELU is frequently utilized in transformer-based models and can improve model performance in applications like image recognition and natural language processing.

## Documentation
[Project Report](https://drive.google.com/file/d/1LuMJcMYoKJTcSaG-YRwG61jorViqScaE/view?usp=sharing)


## Authors

- [@Mayank Raj](mayank.09raj.gmail.com)
- [@Vaidik Chhirolya](vaidik.chhirolya1@gmail.com)
- [@Ankit Tiwari](ankit2392001@gmail.com)

